
\chapter{Expectation Maximisation}

\textbf{NB:} This is based on Chapter 9 of \cite{bishop2006prml}.

\section{Gaussian mixtures}

Consider a dataset $\{x_1,x_2,\ldots,x:N\}\subset\R^d$. Our task is to partition this set into $K\in\N$ subsets; we will consider $K$ known for now.  Intuitively, each subset of points---referred to as a \emph{cluster}---should share some common or similar patterns; a formal definition of similarity in this case will be ignored until needed. 

A natural solution for this segmentation problem is to define $K$ prototypes denoted $\{\mu_1,\mu_2,\ldots,\mu_K\}\subset\R^d$ and determine the assignment of each datapoint $x_n$ to each prototype $\mu_k$, according to a given criterion. 

To solve this optimisation problem, we can define a set of binary variables $\{r_{nk}\}_{nk}\subset\{0,1\}$, where 
\begin{equation}
    r_{nk}=1 \iff x_n\text{ is assigned to } \mu_k. 
\end{equation}

Then, using the Euclidean distance as similarity criterion, the objective can be written as 
\begin{equation}
    J = \sum_{n=1}^{N}\sum_{k=1}^{K} r_{nk} ||x_n-\mu_k||^2.\label{eq:J_k-means}
\end{equation}
The solution to the clustering problem obtained via the minimisation of the loss in eq.~\eqref{eq:J_k-means} is
\begin{align}
r_{nk} &=
\begin{cases}
1, & \text{if } k=\argmin_j||x_n-\mu_j||^2, \\
0, & \text{if not}.
\end{cases} \label{eq:kmeans1}\\
\mu_k &= \frac{\sum_{n=1}^{N}r_{nk}x_n}{\sum_{n=1}^{N}r_{nk}}.\label{eq:kmeans2}
\end{align}

This solution can be calculated by iteratively implementing the above equations, which is known as the $k$-means algorithm. 

\begin{remark}
    Observe that the $k$-means recursion ensures convergence in a finite number of steps: this is because eq.~\eqref{eq:kmeans1} defines a discrete number of solutions, and \eqref{eq:kmeans2} is the global optima for a given $\{r_{nk}\}_{nk}$.
\end{remark}

There are some known drawbacks of $k$-means, for instance 
\begin{itemize}
    \item Speed: computing the assignment variables has a cost $\cO(NK)$. 
    \item It depends on the Euclidean distance that might not be robust to outliers
    \item It only provide hard assignments, not a degree of \emph{responsibility}.  
\end{itemize}

\section{The Gaussian mixture model}

Let us consider the following PGM: 
\begin{equation}
    p(x) = \sum_{k=1}^{K} \pi_k \cN(x; \mu_k,\Sigma_k),
\end{equation}
where $0\leq \pi_k \leq 1, \sum_{k=1}^{K}\pi_k=1$, $\mu_k\in\R^d$ and $\Sigma_k\in\R^{d\times d}$.

This formulation seems to be an improved clustering model wrt $K$-means, since it---at least---allows for learning the shape (variance) of each cluster and admits the definition of a soft assignment variable.

However, note that the likelihood of this models is ill posed. Denoting the parameters by $\theta = \{\pi_{1:K},\mu_{1:K}, \Sigma_{1:K}\}$ and the i.i.d.~data $\x =\{x_1,x_2,\ldots,x_n\}$, the log-likelihood is given by 
\begin{equation}
    l(\theta) = \log p(\x|\theta) = \log\prod_{n=1}^{N} p(x_n|\theta) = \sum_{n=1}^{N} \log p(x_n|\theta) = \sum_{n=1}^{N} \log \sum_{k=1}^{K} \pi_k \cN(x; \mu_k,\Sigma_k).
    \label{eq:likelihood_GMM}
\end{equation}
This objective can reach an infinite value if a Gaussian component is assigned to a single datapoint with a vanishing variance. Additionally, for each possible assignment, there are $K!$ different solutions that provide such assignment.  

We will derive an equivalent formulation to the PGM above that admits a more interpretable and \emph{stepwise} training procedure. To this end,  let us introduce a set of $K$ latent variables $\{z_{k}\}\subset\{0,1\}$, $\sum_{k=1}^K z_{k} = 1$. We can write 
\begin{equation}
    p(x,z) = p(x|z)p(z).
\end{equation}
Also, defining $p(z_{k}=1) = \pi_k$, we can express the pmf/pdf: 
\begin{align}
    p(z) &= \prod_{k=1}^{K} \pi_k^{z_k}\\
    p(x|z) &= \prod_{k=1}^{K} \cN(\mu_k,\Sigma_k)^{z_k},
\end{align}
with the marginal pdf over $x$ as 
\begin{equation}
    p(x) = \sum_{k=1}^{K} p(z_k)p(x|z_k)= \sum_{k=1}^{K} \pi_k \cN(x; \mu_k,\Sigma_k).
\end{equation}
Thus, showing that the formulations are equivalent. 

In this formulation, let us define the \emph{responsibilities} of the $k$-th component to explain the observation $x$ given by
\begin{align}
    \gamma(z_k) \defeq p(z_k = 1|x) 
    &= \frac{p(x|z_k=1)p(z_k=1)}{\sum_{j=1}^{K}p(x|z_j=1)p(z_j=1)}\\
    &= \frac{\pi_k \cN(x; \mu_k,\Sigma_k)}{\sum_{j=1}^{K} \pi_j \cN(x; \mu_j,\Sigma_j)}.
\end{align}

\begin{remark}
    The latent-variable formulation of the GMM allows for direct sampling from that PGM: first sample $z\sim p(z)=\prod_{k=1}^{K}$, and then sample $x\sim p(x|z)= \prod_{k=1}^{K} \cN(\mu_k,\Sigma_k)^{z_k}$. This is known as \emph{ancestral sampling}.
\end{remark}


\section{Expectation Maximisation for GMMs}

We will introduce a learning approach for PGMs that features a latent variable called Expectation Maximisation (EM). We will first present it in the particular case of the GMM model, and then in the general case. 

The first order optimality conditions for the log-likelihood in eq.~\eqref{eq:likelihood_GMM} give
\begin{align}
    \mu_k &= \frac{1}{N_k} \sum_{n=1}^N \gamma(z_{nk})x_n\label{eq:GMM_LL_soln1}\\
    \Sigma_k & = \frac{1}{N_k} \sum_{n=1}^N \gamma(z_{nk})(x_n - \mu_k)(x_n - \mu_k)^\top\label{eq:GMM_LL_soln2}\\
    \pi_k &= \frac{N_k}{N},\label{eq:GMM_LL_soln3}
\end{align}
where we have defined the effective number of samples per component as $N_k = \sum_{n=1}^N \gamma(z_{nk})$.

\begin{exercise}
    Derive eqs.~\eqref{eq:GMM_LL_soln1}-\eqref{eq:GMM_LL_soln3}
\end{exercise}

\begin{remark}
    Observe how the optimal mean and variance of each component is a weighted average of all the data points, where the weights are proportional to the responsibility (contribution) of that component to generation of the sample. Also, note that eqs.~\eqref{eq:GMM_LL_soln1}-\eqref{eq:GMM_LL_soln3} can be considered as the soft-assignment version of the $K$-means solutions, with the additional flexibility of having an learnable expression for the shape of the clusters.  
\end{remark}

Eqs.~\eqref{eq:GMM_LL_soln1}-\eqref{eq:GMM_LL_soln3} do not provide a direct closed-form solution, since they depend on the responsibilities $\gamma(z_{nk})$ which are functions of all the parameters. However, they can still be implemented in following the steps: 
\begin{itemize}
    \item[(E)] Compute $\gamma(z_{nk}) = p(z_{nk}=1|\x)$.
    \item[(M)] Use $\gamma(z_{nk})$ to compute eqs.~\eqref{eq:GMM_LL_soln1}-\eqref{eq:GMM_LL_soln3}. 
\end{itemize}


\section{An interpretation of EM}

Let us now leave the GMM aside. In more general, perhaps abstract, terms, the goal of the  EM algorithm is to find maximum likelihood solutions for latent variable models (LVMs) by breaking down the the optimisation problem into a functional approximation of the likelihood, and then the (simpler) optimisation of such approximation. 

Recall our notation involving an observed variable $x$ and a latent variable $z$. In general LVMs, the likelihood can be expressed as 
\begin{equation}
    p(x|\theta) = \int_\cZ p(x|\theta,z)p(z|\theta)\dz. \label{eq:LVM_LL}
\end{equation}
\textbf{NB:} We treat both the discrete and continuous equivalently.

In general, direct optimisation of eq.~\eqref{eq:LVM_LL} is difficult. Even calculating the above expression is only possible in limited cases, since mixtures do not mix well with the logarithm. In fact, even for likelihood in the exponential family, the mixture is not longer exponential and thus the application of the logarithm does not remove the exponential as in the single-Gaussian case.  

Let us then consider the hypothetical scenario where we have access to the values of $z$ alongside the observed $x$. 

\begin{definition}
    We will refer to $\{\x,\z\}$ as the complete dataset, while $\x$ will be the \emph{observed} or \emph{incomplete} dataset. 
\end{definition}

The related likelihood  to the complete dataset is 
\begin{align}
    \log p(\x,\z|\theta) 
    &= \log \prod_{n=1}^{N} p(x_n,z_n|\theta)\\
    &= \log \prod_{n=1}^{N} p(x_n|z_n,\theta)p(z_n|\theta)\\
    &= \sum_{n=1}^{N} \log  p(x_n|z_n,\theta)  +  \log p(z_n|\theta)\label{eq:complete-data_likelihood}
\end{align}
which we will assume is simpler to evaluate and  optimise than $\log p(\x|\theta)$.

This, however, is impractical since $\z$ is unknown. An interesting interpretation of this optimisation problem is presented next.

\begin{remark}
    Since $z_n$ is not observed, the complete-data log-likelihood in eq.~\eqref{eq:complete-data_likelihood} can be interpreted as a random function. Therefore, an alternative optimisation strategy is to estimate its expectation (E step) and then maximise the resulting deterministic expression (M step). At the end of the chapter, we will formally justify why taking the expectation is more than an intuition. 
\end{remark}

In more detail, this 2-step optimisation procedure results in moving from a candidate solution $\theta^\text{old}$ by first computing $p(\z|\x,\theta^\text{old})$ and then the expectation of the complete-data log-likelihood in eq.~\eqref{eq:complete-data_likelihood} given by 
\begin{equation}
    Q(\theta,\theta^\text{old}) = \sum_{z} \log p(\x,\z|\theta) p(\z|\x, \theta^\text{old}),
\end{equation}
to finally reach an updated candidate solution $\theta^\text{new}$
\begin{equation}
    \theta^\text{new} =  \argmax_\theta  Q(\theta,\theta^\text{old}).
\end{equation} 

\begin{remark}
    This procedure can also be used for maximum a posteriori estimation, in which case $  Q(\theta,\theta^\text{old}) \to   Q(\theta,\theta^\text{old}) + \log p(\theta)$ incorporates the prior over the parameter. 
\end{remark}

Now let us return to the GMM case and feed back these observations. For the GMM, the complete-data log-likelihood is
\begin{equation}
    p(\x,\z|\theta) = \prod_{n=1}^{N}\prod_{k=1}^{K} \pi_k^{z_{nk}}\cN(x_n|\mu_k,\Sigma_k)^{z_{nk}},
\end{equation}
and thus the log-likelihood is
\begin{equation}
    l(\theta) = \sum_{n=1}^{N}\sum_{k=1}^{K} z_{nk} \left(\log \pi_k + \log \cN(x_n|\mu_k,\Sigma_k)\right), \label{eq:complete-data-loglikehood-GM}
\end{equation}
which is tractable. 

\begin{remark}
    The objective in eq.~\eqref{eq:complete-data-loglikehood-GM} is straightforward to optimise: since only one term in the $k$-sum is non-zero, the optimal mean and covariances can be computed in the same ways as the single Gaussian case. Furthermore, imposing the first order optimality condition and enforcing the $\sum_{k=1}^K\pi_k = 1$ via the Lagrangian, gives $\pi_k = \sum_{n=1}^N x_n/N$ directly.
\end{remark}


Recall that this optima is a function of $\z$, and thus impossible to calculate directly, so we will calculate its expectation. To this end, we have
\begin{equation}
    p(\z|\x,\theta) \propto p(\z,\x|\theta) = \prod_{n=1}^{N}\prod_{k=1}^{K} \underbrace{\pi_k^{z_{nk}}\cN(x_n|\mu_k,\Sigma_k)^{z_{nk}}}_{\propto p(z_{nk}|x_n,\theta)},
\end{equation}
which means that $ p(\z|\x,\theta) $ factorises wrt to $n$ and $k$, and thus all the $z_n$ are independent. This is reasonable, since the cluster responsibilities over one sample should not affect the rest (due to the i.i.d.~assumption).

The expectation of $\z$ can be computed as follows, 
\begin{align}
    \E{z_{nk}} 
    &= 1\cdot p(z_{nk} = 1 | \x,\theta) + 0\cdot p(z_{nk} = 0 | \x,\theta)\\
    &=  p(z_{nk} = 1 | \x,\theta)\\
    &=  \gamma(z_{nk})\\
    &\defeq \frac{\pi_k \cN(x_n|\mu_k,\Sigma_k)}{\sum_{j=1}^{K}\pi_j \cN(x_n|\mu_j,\Sigma_j)}.
\end{align}
Note from eq.~\eqref{eq:complete-data-loglikehood-GM} that the objective is linear in $z_{nk}$, which makes the computation of its expectation straightforward:
\begin{equation}
    \mathbf{E}_z \log p(\x,\z|\theta) = \sum_{n=1}^{N}\sum_{k=1}^{K} \gamma(z_{nk}) \left(\log \pi_k + \log \cN(x_n|\mu_k,\Sigma_k)\right).
\end{equation}

\begin{exercise}
    Show that GMM recovers $K$-means. For that, choose $p(x|\mu_k,\Sigma_k) = \cN(x|\mu_k,\epsilon I)$ with $\epsilon>0$ fixed to show that you recover a \emph{soft-assignment} version of $K$-means. Then, take $\epsilon\to 0$ to recover vanilla $K$-means.
\end{exercise}

\begin{exercise}
    See the applications of EM to mixtures of Bernoulli and Bayesian linear regression in \cite{bishop2006prml}.
\end{exercise}

\begin{example}
    Let us consider an implementation of the GMM training pipeline as described. Assuming $\R^2$ as the sample space, $K=3$ clusters, $N=400$ samples, Fig.~\ref{fig:GMM_init} shows two choices for the initial condition. Then, after running the iterative training procedure, Fig.~\ref{fig:GMM_final} shows the learnt clusters alongside the true values. Lastly, Fig.~\ref{fig:GMM_vs} shows the evolution of the likelihood per iteration. Note that both initialisations arrived at the same model, but the \emph{good} initialisation was more efficient. The demo is available in the repository. 

    \begin{figure}
        \centering
        \includegraphics[width=0.48\textwidth]{img/week2_GMM_good_init.pdf}
        \hfill
        \includegraphics[width=0.48\textwidth]{img/week2_GMM_bad_init.pdf}
        \caption{Two initialisations for the GMM model}
        \label{fig:GMM_init}
    \end{figure}

    \begin{figure}
        \centering
        \includegraphics[width=0.48\textwidth]{img/week2_GMM_good_final.pdf}
        \hfill
        \includegraphics[width=0.48\textwidth]{img/week2_GMM_bad_final.pdf}
        \caption{Solutions corresponding to the initialisations in Fig.~\ref{fig:GMM_init}.}
        \label{fig:GMM_final}
    \end{figure}

        \begin{figure}
        \centering
        \includegraphics[width=0.55\textwidth]{img/week2_GMM_good_vs_bad.pdf}
        \caption{Evolution of the log-likelihoodfor both initialisations in Fig.~\ref{fig:GMM_init}.}
        \label{fig:GMM_vs}
    \end{figure}
\end{example}

\section{EM in its general form}

Recall: 
\begin{equation}
    \underbrace{p(\x|\theta)}_{\text{difficult}} = \sum_\z \underbrace{p(\x,\z|\theta)}_{\text{easier}}.
\end{equation}
We are interested in deriving EM as a model-approximation approach. To that end, let us consider a distribution over the latent variable $q(z)$; intuitively, this distribution will approximate $p(z|x,\theta)$. For any choice of $q$, the following holds: 

\begin{align*}
    \log p(x|\theta) 
    &=  \sum_{z} q(z) \log p(x|\theta) \quad &&\leftarrow \quad \log p(x|\theta) \text{is constant wrt } z\\
    &=  \sum_{z} q(z) \log \left(p(x|\theta) \frac{p(z|x,\theta)q(z)}{p(z|x,\theta)q(z)}\right)  \quad &&\leftarrow \quad \text{multiply by 1}\\
    &=  \sum_{z} q(z) \log \left( \frac{p(x,z|\theta)}{q(z)}\cdot \frac{q(z)}{p(z|x,\theta)} \right)  \quad &&\leftarrow \quad \text{arrange}\\
    &= \underbrace{\sum_{z} q(z) \log \left( \frac{p(x,z|\theta)}{q(z)} \right)}_{\cL(q,\theta)} + \underbrace{\sum_{z} q(z) \log \left( \frac{q(z)}{p(z|x,\theta)} \right)}_{\KL{q(z)}{p(z|x,\theta)}}.  \quad &&\leftarrow \quad \text{split}\\
\end{align*}

\begin{remark}
    Recall that the KL is always non-negative, meaning that $\cL(q,\theta)$ is a lower bound for $\log p(x|\theta)$.
\end{remark}

EM breaks the ML problem into two simpler problems related to the computation of a lower bound and its optimisation. The objective above can be minimised in two stages: 
\begin{itemize}
    \item First, the distribution $q$ is chosen in order to minimise the term $\KL{q(z)}{p(z|x,\theta)}$, where, the optimal solution in $q(z) = p(z|x,\theta)$.
    \item Then, using that choice for $q$, the term $\cL(q,\theta)$ is optimised. Notice that this term is an expectation  wrt $q$.
\end{itemize}

\begin{remark}
     This view formalises the intuition that we presented for the GMM case. The choice of the expectation of the complete log-likelihood is not arbitrary, but follows from finding the optimal approximating distribution in the KL sense.
\end{remark}



\paragraph{Monotonicity of EM.}
Recall the variational decomposition (valid for any distribution $q$ on $z$)
\begin{equation}
\label{eq:em-decomp}
\log p(x | \theta)
= \mathcal L(q,\theta) + \KL{q(z)}{p(z | x,\theta)},
\end{equation}
where
\begin{equation}
\label{eq:elbo-def}
\cL(q,\theta)
:= \sum_z q(z)\log\frac{p(x,z| \theta)}{q(z)}.
\end{equation}
Since $\KL{\cdot}{\cdot}\ge 0$, we have $\log p(x| \theta)\ge \cL(q,\theta)$ for all $(q,\theta)$.

Let $\theta^{t}$ be the current iterate. The E-step sets
\begin{equation}
\label{eq:e-step}
q^{t+1}(z) := p(z| x,\theta^{t}),
\end{equation}
for which $\KL{q^{t+1}(z)}{p(z\mid x,\theta^{t})}=0$. Plugging into \eqref{eq:em-decomp} yields
\begin{equation}
\label{eq:tight-at-theta-t}
\log p(x| \theta^{t}) = \cL(q^{t+1},\theta^{t}).
\end{equation}
The M-step then chooses
\begin{equation}
\label{eq:m-step}
\theta^{t+1} \in \arg\max_{\theta}\; \cL(q^{t+1},\theta),
\end{equation}
hence
\begin{equation}
\label{eq:increase-elbo}
\mathcal L(q^{t+1},\theta^{t+1}) \ge \cL(q^{t+1},\theta^{t})
= \log p(x| \theta^{t}).
\end{equation}
Finally, since $\log p(x| \theta)\ge \cL(q^{t+1},\theta)$ for any $\theta$, we obtain
\begin{equation}
\label{eq:em-monotone}
\log p(x| \theta^{t+1})
\;\ge\; \mathcal L(q^{t+1},\theta^{t+1})
\;\ge\; \log p(x| \theta^{t}).
\end{equation}
Therefore EM produces a non-decreasing sequence of log-likelihood values.

\begin{remark}[Generalised EM]
The monotonicity argument above only requires that the M-step returns
$\theta^{t+1}$ such that $\cL(q^{t+1},\theta^{t+1})\ge \cL(q^{t+1},\theta^{t})$.
Thus, it is not necessary to fid the exact (local) optimum of $\cL$ in the M-step; any update that increases
$\cL$ yields a \emph{generalised EM} (GEM) procedure with the same monotonicity guarantee. This observation will be key for the variational inference part.
\end{remark}

\paragraph{Does EM fix ill-posed maximum likelihood?}
For several latent-variable models, the maximum likelihood (ML) objective is \emph{ill-posed}, since the log-likelihood can be unbounded. A canonical example is the Gaussian mixture model (GMM): as we saw in class, for any datapoint $x_n\in\R^d$ and any $\epsilon>0$, consider a mixture component $k$ with
\[
\mu_k = x_n,
\qquad
\Sigma_k = \epsilon I.
\]
Then
\[
\mathcal N(x_n;\mu_k,\Sigma_k)
= (2\pi)^{-d/2}|\Sigma_k|^{-1/2}
= (2\pi)^{-d/2}\epsilon^{-d/2},
\]
so the corresponding contribution to $\log p(x_n| \theta)$ diverges as $\epsilon\to 0$.
Consequently, $\sup_{\theta}\log p(x| \theta)=+\infty$ for unconstrained GMM maximum likelihood.

EM \emph{does not} resolve this ill-posedness, because it is still (attempting to) maximise the same ML
objective; it only changes the optimisation route. In fact, the EM updates can actively move toward
singular solutions:
if a component collapses around a datapoint, the E-step tends to assign it responsibility close to $1$
for that datapoint, and the M-step covariance update (a responsibility-weighted empirical covariance)
can shrink toward a rank-deficient matrix unless additional constraints are imposed.

\begin{remark}[How to avoid degeneracy]
To obtain well-posed estimation one typically modifies the objective or the parameter space, e.g.
\begin{itemize}
\item \textbf{MAP (penalised) EM:} maximise $\log p(x\mid \theta)+\log p(\theta)$ by placing a prior on
      parameters (e.g.\ inverse-Wishart prior on $\Sigma_k$, Dirichlet prior on mixing weights).
\item \textbf{Constrained ML:} enforce $\Sigma_k\succeq \sigma_{\min}^2 I$ or tie covariances (e.g.\ $\Sigma_k=\Sigma$).
\item \textbf{Regularisation / damping:} add a penalty to discourage small determinants, or replace $\Sigma_k\leftarrow \Sigma_k+\epsilon I$ after updates.
\item \textbf{Alternative components:} heavier-tailed mixtures (e.g.\ Student-$t$ mixtures) can reduce single-point capture.
\end{itemize}
\end{remark}

\noindent
In short: EM guarantees non-decreasing likelihood; when the likelihood is \emph{unbounded above},
monotone ascent is still compatible with divergence toward a singular solution.

\begin{remark}
EM does not change the (possibly ill-posed) ML objective; it only changes the optimisation procedure.
In particular, it replaces direct maximisation of $\log p(x| \theta)$ by alternating easier subproblems (posterior inference and complete-data fitting) and guarantees monotone progress, which often stabilises learning in practice even though degeneracies may still exist in principle.
\end{remark}


\section{Concluding remarks}

\begin{itemize}
\item \textbf{EM as coordinate ascent on a lower bound.}
      EM alternates between (i) an \emph{inference} step, selecting
      $q(z)=p(z| x,\theta)$ to tighten a variational lower bound, and
      (ii) a \emph{learning} step, updating $\theta$ to increase that bound:
      \[
      q^{t+1}(z)=p(z| x,\theta^t),
      \qquad
      \theta^{t+1}\in\arg\max_{\theta}\; \mathbf{E}_{q^{t+1}}[\log p(x,z| \theta)].
      \]
\item \textbf{Inference vs.\ learning.}
      Latent-variable models are expressive, but at the cost of posterior inference.
      EM addresses this inference need, where the E-step performs posterior computation and the M-step performs parameter fitting.
\item \textbf{Feasibility of EM.}
      EM is particularly effective when the complete-data likelihood belongs to a tractable family (often exponential-family),
      leading to closed-form M-steps. 
      When the exact posterior is intractable, one can replace the E-step by a restricted family $q$ (variational EM).
\item \textbf{Limitations.}
      EM is monotone but generally converges to a stationary point, so performance depends on initialization.
      Moreover, EM does not fix ill-posed ML objectives (e.g.\ degeneracy in GMMs) without additional constraints or priors.
\end{itemize}

\noindent
These ideas generalise beyond mixtures: many modern learning procedures can be understood as
optimising tractable surrogates (bounds) of intractable objectives, with EM providing a canonical template.


\subsection{Suggested exercises}

\begin{enumerate}
    \item \textbf{Latent variable models and incomplete data (theory).}  
    \begin{enumerate}
        \item Define a latent variable model and distinguish between complete and incomplete data.
        \item Explain why direct maximisation of the marginal likelihood is often intractable.
        \item Describe how the introduction of latent variables simplifies modelling but complicates inference.
    \end{enumerate}

    \item \textbf{Expectationâ€“Maximisation algorithm  (theory).}  
    Consider a latent variable model with parameters $\theta$.
    \begin{enumerate}
        \item Derive the EM algorithm starting from the marginal log-likelihood.
        \item Define the $Q$-function and explain the role of the E-step and the M-step.
        \item Prove that each EM iteration does not decrease the log-likelihood.
    \end{enumerate}
    \item \textbf{Gaussian mixture models (coursework).}  
    \begin{enumerate}
        \item Implement the EM algorithm for Gaussian mixture models.
        \item Investigate the effect of initialisation on convergence.
        \item Illustrate the relationship between $k$-means and GMMs by varying the covariance structure.
    \end{enumerate}

    \item \textbf{Likelihood degeneracy and regularisation (coursework).}  
    \begin{enumerate}
        \item Demonstrate empirically the likelihood degeneracy of Gaussian mixture models.
        \item Propose and implement at least one regularisation strategy.
        \item Analyse how regularisation affects the learned parameters and likelihood.
    \end{enumerate}
\end{enumerate}

