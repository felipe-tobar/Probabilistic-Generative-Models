\chapter{Optimal Transport}

\textbf{NB:} This is based on \cite[Ch.~1-2]{peyre_cuturi_2019} and \cite{kingma2013auto}.

\section{Motivation}

In many applications within generative modelling, we need to compare probability distributions in a way that respects the geometry of the sample space. Classical discrepancies such as $\ell_p$ distances between histograms or $f$-divergences (e.g.\ KL, JS) are often
referred to as \emph{vertical distances}, as they compare distributions in a pointwise fashion, ignoring the notion of distance between locations in the sample space.

\begin{itemize}
    \item \textbf{Drawbacks of vertical distances.}
    Vertical distances become uninformative when the supports of the distributions do not overlap: two distributions with disjoint supports can be arbitrarily far apart or even incomparable, regardless of how close their supports are in space.

    \item \textbf{Sense of proximity and convergence.}
    When learning generative models, we expect distributions supported on nearby regions to be considered close. For instance, if $x_n \to x$ in $\mathbb{R}^d$, the Dirac measures $\delta_{x_n}$ should converge to $\delta_x$. This notion of convergence is not captured by most classical divergences.

    \item \textbf{Lifting the base distance.}
    Optimal transport provides a principled way to lift a ground distance
    $d(\cdot,\cdot)$ on the sample space to a distance between probability
    distributions by explicitly accounting for the cost of transporting mass across space. This leads to geometrically meaningful distances on spaces of measures, known as Wasserstein distances.
\end{itemize}

\section{The Monge problem}

Let us consider the assignment problem, that is, how to allocate a group of items from a set of source locations to a set of target locations. 

\begin{example}[The vineyard problem]
    Consider a wine production company, where grapes produced from different vineyards must be transported to processing plants. An optimal assignment in this case must consider the distance between each vineyard and processing plant, as well as their  production and processing capacities respectively, to achieve the minimum transport cost. See Fig.~\ref{fig:wine_assignment} for an illustration.
\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{img/week5_wine_assignment.pdf}
    \caption{Illustration of the vineyard problem.}
    \label{fig:wine_assignment}
\end{figure}
\end{example}

We will consider both discrete and continuous distributions on the items to be assigned (or \emph{transported}); we recall them as follows. 

\begin{definition}[Discrete measure with finite support]
Let $\cX$ be a set. A measure $\mu$ is \emph{discrete with finite support}
if there exist points $x_1,\dots,x_n \in \mathcal X$ and weights
$w_1,\dots,w_n \ge 0$ such that, for any subset $A \subseteq \cX$,
\[
\mu(A) = \sum_{i=1}^n w_i\,\mathbf{1}_{\{x_i \in A\}}.
\]
Equivalently,
\[
\mu = \sum_{i=1}^n w_i\,\delta_{x_i}.
\]
The measure $\mu$ is a probability measure if $\sum_{i=1}^n w_i = 1$.
\end{definition}


\begin{definition}[General measure with density]
Let $\mathcal X \subseteq \mathbb{R}^d$. A measure $\mu$ is said to
\emph{admit a density} if there exists a nonnegative function
$\rho : \mathcal X \to [0,\infty)$ such that, for any subset $A \subseteq \mathcal X$,
\[
\mu(A) = \int_A \rho(x)\,dx.
\]
The measure $\mu$ is a probability measure if
\[
\int_{\mathcal X} \rho(x)\,dx = 1.
\]
\end{definition}


Let us consider two discrete distributions $\mu$ and $\nu$, given by 
\begin{equation}
    \mu = \sum_{i=1}^{N}\mu_i\delta_{x_i},\quad \text{and}\quad \nu = \sum_{j=1}^{M} \nu_j\delta_{y_j}.
\end{equation} 
The discrete Monge formulation finds a map, denoted $T$, that associates each source point $x_i$ with a single point target $y_j$ in order to allocate the mass from $\mu$ to $\nu$. Note that more than one source point can be allocated to the same target, meaning that the map is surjective (onto the set with $\nu_j>0$) yet not necessarily injective. The map $T: \{x_1,x_2,\ldots,x_N\}\to \{y_1,y_2,\ldots,y_M\}$, must verify
\begin{equation}\label{eq:Monge_PF}
 \nu_j = \sum_{i:T(x_i)=y_j}\mu_i,\quad \forall j=1,\ldots,M,
\end{equation}
meaning that $\nu$ is the \emph{push-forward measure} of $\mu$ through $T$. 

Among the possible maps fulfilling eq.~\eqref{eq:Monge_PF}, Monge's formulation minimises a given transportation cost defined over the source-target pairs. Denoting the transport cost by $c:\{x_1,x_2,\ldots,x_N\}\times \{y_1,y_2,\ldots,y_M\} \to \R_+$, the optimal map is
\begin{equation}
    T^* = \argmin_{T_\#\mu=\nu}\, \sum_{i=1}^N c(x_i,T(x_i)).   
\end{equation}

\begin{example}[The assignment problem]
    When $N=M$ and all masses are equal, that is, $\mu_i = \nu_j = 1/N, \forall i,j$, the mass conservation constraint implies that $T$ is bijective. 
\end{example}

\begin{remark}[Existence and uniqueness]
    Monge maps may not exist in the general case. In particular, if $M>N$, a Monge map cannot exist since mass cannot be split. Furthermore, depending on the cost function, multiple maps can achieve the same optimal cost. See Fig.~\ref{fig:Monge_existence_uniqueness} for an illustration.

    \begin{figure}
    \centering
    \includegraphics[width=0.75\textwidth]{img/week5_existence_uniqueness.pdf}
    \caption{Monge's map may be non-unique (left) or may not even exist (right) in particular cases.}
    \label{fig:Monge_existence_uniqueness}
\end{figure}
\end{remark}

The Monge problem can also be formulated for general (continuous and/or discrete) measures. Let us consider two measures $\mu$ and $\nu$ supported on spaces $\cX$ and $\cY$ respectively, and a cost function $c:\cX\times\cY\to\R_+$.  In this case, the Monge map is given by 

\begin{equation}
    T^* = \argmin_{T_\#\mu=\nu}\, \int_{\cX} c(x,T(x))\d\mu(x).   
\end{equation}

See Fig.~\ref{fig:monge_diagram} for an illustration.\footnote{Infinite thanks to Elsa Cazelles (IRIT, CNRS) for kindly sharing these beautiful \texttt{tikz} figures.}


\begin{figure}
    \centering
    \begin{tikzpicture}[>=stealth,scale=1.75]
    \small
    \blobA[dashed]{a}{0,0}
    \blobB[dashed]{b}{3,0}
    \node at(0.2,0) {$\cX$};
        \node at(3.2,0) {$\cY$};
    \draw[->] (1.8,1) to[bend left] node[above]{$T$} (3.1,1); 
    \node at(1.5,-0.5) {\footnotesize $A=\{x\in\cX:T(x)\in B\}$};
    \node at(3.55,0) {\scriptsize $B$};
    \draw[->] (0.75,-0.4) to (1.05,-0.05);   
    \node at(6,0.5) {$T\#{\color{red!50} \mu} = {\color{blue!50} \nu}$};
    \node at(6.3,0.2) {i.e., ${\color{red!50} \mu(A)} = {\color{blue!50} \nu(B)}$};
    \end{tikzpicture}
    \caption{Illustration of the Monge map for continuous measures,  adapted from \protect\cite{thorpe_2018_intro_ot}.}
    \label{fig:monge_diagram}
\end{figure}

We will be particularly interested when the measures involved are probability measures associated to random variables $X$ and $Y$, this is because we want to rely on the theory of mass transport to construct generative models. However, note that in Monge's standard formulation this is very limiting, since we cannot deal with general cases beyond the histograms with the same number of atoms and uniform weights. In particular, modelling the association between the supports of $\mu$ and $\nu$ by the map $T$, does not allow for mass splitting. 


\section{Kantorovich relaxation}

To allow for the mass of $\mu$ at a given location to be split and then transported to different locations to match $\nu$, Kantorovich's formulation relaxes the deterministic nature of Monge's transport, considering a probabilistic transport instead.

We now return to the discrete measure setting with measures $\mu$ and $\nu$. Rather than assuming the existence of a transport map, we will consider a transport plan (or coupling) $P\in\R^{N\times M}$, where $P_{i,j}$ describes the amount of mass that goes from $x_i$ to $y_j$. The condition of the admissible maps fulfilling the pushforward condition, turns into the following condition for the set of admissible plans: 
\begin{equation}
    \Pi(\mu,\nu) \defeq \{ P\in\R_+^{N\times M}, \text{s.t.,} P\one_M = \mu\quad \text{and}\quad P^\top\one_N = \nu  \},
\end{equation}
where $\one_M\in\R^M$ denotes a column vector of ones of dimension $M$. 
Note that, unlike the map formulation, the set of couplings always symmetric, i.e., $P\in\Pi(\mu,\nu) \iff P^\top\in\Pi(\nu, \mu)$.

\begin{remark}
    The set $\Pi(\mu,\nu)$ is defined by $N+M$ equality constraints and this is a convex polytope, that is, the convex hull of a finite set of matrices. The vertices of this polytope will be of particular interest.  
\end{remark}

Then, considering a cost matrix $C_{ij} = c(x_i,y_j)$, Kantorovich's optimal transport cost between discrete measures can be defined as 
\begin{equation}\label{eq:kantorovich_discrete}
    L(\mu,\nu) \defeq \min_{P\in\Pi(\mu,\nu)}
 \langle C,P\rangle = \sum_{i=1}^{N}\sum_{j=1}^{M} C_{ij}P_{ij}.
\end{equation}

\begin{example}[Vineyards and wineries]
To develop an intuition of Kantorovich's problem suppose an operator manages $N$ vineyards and $M$ wine processing plants. Each vineyard, indexed by $i$, produces $\mu_i$ units of grapes
during the harvest season. These grapes must be transported in their entirety to the processing plants, where each plant $j$ requires exactly $\nu_j$ units of grapes in order to operate at full capacity.

To transport grapes from vineyard $i$ to processing plant $j$, the operator relies on a logistics provider that charges a cost $C_{i,j}$ per unit of grapes transported along that route. The pricing scheme is linear and uniform: shipping $a$ units of grapes from vineyard $i$ to plant \(j\) incurs a total cost of $a\,C_{i,j}$.

The goal of the operator is to determine how much grape mass should be sent from each vineyard to each processing plant so as to satisfy all supply and demand constraints while minimising the total transportation cost.

See Fig.~\ref{fig:Kantorovich_diagram} for an illustration. 
\end{example}

\begin{figure}
    \centering
    \includegraphics[width=0.4\textwidth]{img/week5_wine_assignment_split.pdf}
    \vspace{2em}

    \includegraphics[width=0.9\textwidth]{img/week5_kantorovich.pdf}
    \caption{Illustration of Kantorovich's formulation: mass splitting (top) and transport plan with cost matrix (bottom).}
    \label{fig:Kantorovich_diagram}
\end{figure}

\begin{remark}
    Observe that $\Pi(\mu,\nu)$ always has at least one element, given by $\mu \otimes \nu$, known as the independent coupling. Therefore, Kantorovich's formulation always has a solution.
\end{remark}

\vspace{2cm}

\begin{remark}[Equivalence between Monge and Kantorovich]
Whenever the Monge problem admits a solution, the Kantorovich formulation is equivalent to it. More precisely, if there exists an optimal transport map \(T:\cX\to\cY\) pushing \(\mu\) onto \(\nu\), then the associated transport plan \(\pi_T \in \Pi(\mu,\nu)\), defined by
\[
\pi_T(x,y) = \mu(x)\,\mathbf{1}_{\{y=T(x)\}},
\]
is an optimal solution of the Kantorovich problem and both formulations attain the same optimal cost. In this case, the optimal Kantorovich plan is supported on the graph of the map \(T\).

Conversely, when no optimal transport map exists, the Monge problem is ill-posed, while Kantorovich’s relaxation remains well-defined and admits at least one solution, possibly involving mass splitting.
\end{remark}

To extend Kantorovich's formulation to general (continuous and/or discrete)
measures, let \(\mu\) and \(\nu\) be two measures supported on spaces \(\cX\) and
\(\cY\), respectively, and let \(c:\cX\times\cY\to\R_+\) be a cost function. A
\emph{transport plan} is now a measure \(\pi\) on the product space
\(\cX\times\cY\) whose marginals coincide with \(\mu\) and \(\nu\), that is,
\[
\int_{\cY} \pi(x,y)\,dy = \mu(x),
\qquad
\int_{\cX} \pi(x,y)\,dx = \nu(y).
\]
The Kantorovich optimal transport cost is then defined as
\begin{equation}\label{eq:kantorovich_continuous}
    L(\mu,\nu)
\defeq
\min_{\pi\in\Pi(\mu,\nu)}
\int_{\cX\times\cY} c(x,y)\, d\pi(x,y),
\end{equation}
where \(\Pi(\mu,\nu)\) denotes the set of all admissible couplings between \(\mu\)
and \(\nu\). Unlike the Monge formulation, this problem always admits at least one
solution under mild assumptions on \(c\).


\begin{theorem}[Brenier]
Let $\mu$ and $\nu$ be probability measures on $\mathbb{R}^d$ with finite second
moments. Assume that $\mu$ admits a density with respect to the Lebesgue measure,
and consider the quadratic cost
\[
c(x,y) = \frac{1}{2}\|x-y\|^2.
\]
Then the Kantorovich problem between $\mu$ and $\nu$ admits a unique optimal
solution. Moreover, this optimal transport plan is induced by a map
$T:\mathbb{R}^d \to \mathbb{R}^d$ of the form
\[
T(x) = \nabla \varphi(x),
\]
where $\varphi:\mathbb{R}^d \to \mathbb{R}$ is a convex function. In particular,
$T$ is the unique optimal solution of the Monge problem.
\end{theorem}

\begin{corollary}[Brenier’s theorem in one dimension]
Let $\mu$ and $\nu$ be probability measures on $\mathbb{R}$, and assume that
$\mu$ admits a density. Consider the quadratic cost $c(x,y) = (x-y)^2$.
Then the unique optimal transport map from $\mu$ to $\nu$ is given by the
\emph{monotone rearrangement}
\[
T(x) = F_\nu^{-1}(F_\mu(x)),
\]
where $F_\mu(x) = \mu((-\infty,x])$ and $F_\nu^{-1}(t) = \inf\{y\in\mathbb{R} : F_\nu(y)\ge t\}$
denote the cumulative distribution function of $\mu$ and the quantile function
of $\nu$, respectively. The corresponding Kantorovich optimal plan is supported
on the graph of $T$.
\end{corollary}



\section{Calculating OT}

\paragraph{Particular case: Gaussian measures.}
Let \(\mu=\mathcal{N}(m_\mu,\Sigma_\mu)\) and \(\nu=\mathcal{N}(m_\nu,\Sigma_\nu)\) be Gaussian distributions on \(\mathbb{R}^d\) with positive definite covariance matrices, and consider the quadratic cost \(c(x,y)=\|x-y\|_2^2\). Since these measures admit densities, the Monge problem admits a unique solution. In this case, the optimal
transport map has a closed-form expression and is affine, given by
\[
T(x) = m_\nu + A(x-m_\mu),
\qquad
A = \Sigma_\mu^{-1/2}
\left(\Sigma_\mu^{1/2}\Sigma_\nu\Sigma_\mu^{1/2}\right)^{1/2}
\Sigma_\mu^{-1/2}.
\]
The corresponding optimal transport cost, also known as the squared
\(2\)-Wasserstein distance between \(\mu\) and \(\nu\), decomposes into a term
measuring the distance between the means and a term measuring the discrepancy
between the covariances:
\[
W_2^2(\mu,\nu)
=
\|m_\mu-m_\nu\|_2^2
+
\mathrm{Tr}\!\left(
\Sigma_\mu+\Sigma_\nu
-
2\left(\Sigma_\mu^{1/2}\Sigma_\nu\Sigma_\mu^{1/2}\right)^{1/2}
\right).
\]
In the one-dimensional case, this simplifies considerably: the optimal transport
map reduces to a simple rescaling and translation,
\(T(x)=m_\nu+\frac{\sigma_\nu}{\sigma_\mu}(x-m_\mu)\), and the transport cost
admits a closed-form expression in terms of the means and variances.


\paragraph{Particular case: 1-dimensional measures.} Following from Brenier theorem, a relevant feature of optimal transport is that, in one dimension, the problem admits a closed-form solution. Let $\mu$ and $\nu$ be probability measures on $\R$, e.g., as illustrated in Fig.~\ref{fig:1d_data}, and assume the $L_p$ distance as the associated ground cost. The optimal transport map is given by the \emph{monotone
rearrangement}, which matches equal quantiles of the two distributions. 

\begin{figure}[h]
    \centering
    \includegraphics[scale=0.5]{img/week5_1dim_data.pdf}
    \caption{Two one-dimensional probability distributions. Optimal transport
    matches points with equal cumulative mass.}
    \label{fig:1d_data}
\end{figure}

Denote by $F_\mu$ the cumulative distribution function of $\mu$, and by
$F_\mu^{-}:[0,1]\to\mathbb{R}$ its inverse (quantile function), defined as
\[
F_\mu^{-}(t) = \inf\{x\in\mathbb{R} : F_\mu(x)\ge t\}, \qquad t\in[0,1].
\]
Then, for any $p\ge 1$, the optimal transport cost between $\mu$ and $\nu$ admits the explicit expression
\[
\text{OT}^p(\mu,\nu)
=
\int_0^1 \bigl(F_\mu^{-}(t)-F_\nu^{-}(t)\bigr)^p\,dt
=
\|F_\mu^{-}-F_\nu^{-}\|_{L^p([0,1])}^p.
\]
This representation, illustrated in Fig.~\ref{fig:1d_quantiles}, shows that
one-dimensional optimal transport reduces to measuring the $L^p$ distance
between quantile functions.

\begin{figure}[h]
    \centering
    \includegraphics[scale=0.5]{img/week5_1dim_data_quant.pdf}
    \caption{Quantile functions of the two distributions. The optimal transport cost corresponds to the $L^p$ distance between them.}
    \label{fig:1d_quantiles}
\end{figure}

The same monotone matching principle applies to discrete one-dimensional
measures. In this case, optimal transport is obtained by sorting the atoms of
$\mu$ and $\nu$ and matching them in increasing order, as illustrated in
Fig.~\ref{fig:1d_discrete}.

\begin{figure}[h]
    \centering
    \includegraphics[scale=0.8]{img/week5_one_dim_discrete.pdf}
    \caption{Optimal transport between discrete one-dimensional measures:
    atoms are matched according to their cumulative mass.}
    \label{fig:1d_discrete}
\end{figure}


\paragraph{General case through linear programming.}
Kantorovich's problem is defined by the linear programme in eqs.~\eqref{eq:kantorovich_discrete} or \eqref{eq:kantorovich_continuous}. Recall that the feasible set of the discrete optimal transport problem is the convex and compact transport polytope $\Pi(\mu,\nu)$, defined by the linear
equality and inequality constraints. Since the objective function
$\langle C,P\rangle$ is linear in $P$, the optimal plan is found at an extreme point (vertex) of the polytope. As a consequence, OT plans (in the discrete setting) are typically sparse. See Fig.~\ref{fig:polyotope1} for an illustration.

\begin{figure}
    \centering
    \includegraphics[width=0.4\textwidth]{img/week5_polytope1.pdf}
    \caption{Illustration of the transport polytope and the attained optimal plan.}
    \label{fig:polyotope1}
\end{figure}

\begin{remark}[Intuition into the simplex method]
The simplex method is a classical algorithm for solving linear programs, usually used to solve Kantorovich's OT formulation. It operates by moving along the edges of the feasible polytope from one vertex to another, at each step decreasing the value of the objective function, until no adjacent vertex yields further improvement. Since the optimum of a linear program is attained at a vertex of the feasible set, the simplex method
terminates at an optimal solution after visiting only a (typically small) subset of all vertices.
\end{remark}


\begin{algorithm}[H]
\caption{Discrete optimal transport via a linear program (no toolbox)}
\label{alg:kantorovich}
\begin{algorithmic}[1]
\Require Weights $\mu\in\mathbb{R}^n_+$, $\nu\in\mathbb{R}^m_+$ with $\sum_i \mu_i=\sum_j \nu_j$; cost matrix $C\in\mathbb{R}^{n\times m}$
\Ensure Optimal transport plan $P^\star\in\mathbb{R}^{n\times m}_+$ and cost $OT(\mu,\nu)$
\State \textbf{Define optimisation variables:} $P\in\mathbb{R}^{n\times m}$
\State \textbf{Form constraints (transport polytope):}
\[
P \ge 0,\qquad P\mathbf{1}_m=\mu,\qquad P^\top \mathbf{1}_n=\nu
\]
\State \textbf{Define objective:}
\[
\min_{P}\ \langle C,P\rangle \;\defeq\; \sum_{i=1}^n\sum_{j=1}^m C_{ij}P_{ij}
\]
\State \textbf{Solve the linear program} with any generic LP solver (simplex / interior-point)
\State \textbf{Return} $P^\star$ (an optimal feasible plan) and $OT(\mu,\nu)=\langle C,P^\star\rangle$
\end{algorithmic}
\end{algorithm}

Algorithm \ref{alg:kantorovich} presents the procedure to solve the OT problem. For illustration, Figs.~\ref{fig:kantorovich_example_discrete} and \ref{fig:kantorovich_example_continuous} show implementations of this algorithm. 


\begin{figure}
    \centering
    \includegraphics[height=0.4\textwidth]{img/week5_kantorovich_discrete_histogram.pdf}
    \includegraphics[height=0.4\textwidth]{img/week5_kantorovich_discrete_solution.pdf}
    \caption{Computation of optimal plan (discrete): Source and target discrete distributions (left) and the optimal plan (right) for a given cost matrix. Observe how the plan is sparse, meaning that the mass at each source location is split into only a few target locations.}
    \label{fig:kantorovich_example_discrete}
\end{figure}


\begin{figure}
    \centering
    \includegraphics[width=0.6\textwidth]{img/week5_kantorovich_continuous_density.pdf}
    \includegraphics[width=0.3\textwidth]{img/week5_kantorovich_continuous_solution.pdf}
    \caption{Computation of optimal plan between Gaussian mixtures using the $L_2$ cost (discrete, but more dense): Source and target discrete distributions (left) and the optimal plan (right) for a given cost matrix. Observe how the plan seems extremely sparse with the plans almost completely supported on the plot of a function. Though this a discrete example, since these mixtures of Gaussians are absolutely continuous, the OT cost is unique and equal to Monge's solution (Brenier thm), which can seen from the plot. Furthermore, also in line with Brenier thm,  note that the transport is monotonic, meaning transported masses \emph{do not cross paths}.}    
    \label{fig:kantorovich_example_continuous}
\end{figure}

\begin{remark}[Drawbacks of the linear program]
    Kantorovich's formulation allows for a well-posed optimisation problem with attractive uniqueness and existence properties, and numerically stable solutions. However, there are two main drawback of addressing the computation of the  OT via linear programming: first, the computational cost becomes prohibitive when the number of atoms grow beyond $\approx10^4$. Second, solutions are sparse, quasi-deterministic, matrices given at the vertices of $\Pi(\mu,\nu)$.
\end{remark}

\paragraph{Entropy-regularised OT (Sinkhorn).}

To address the computational limitations of linear programming approaches,
a popular strategy is to add an entropic regularisation term to the
Kantorovich problem, leading to fast iterative algorithms.
This approach was popularised in machine learning by \cite{cuturi_2013_sinkhorn}.


For $\alpha\in\R_+$, define
\begin{equation}
    \Pi_\alpha(\mu,\nu) = \left\{ \pi\in\Pi(\mu,\nu) : \KL{\pi}{\mu\otimes\nu}\leq\alpha    \right\},
\end{equation}
and consider the constrained OT problem wrt the cost $C$:
\begin{equation}
    \pi_\alpha^\star = \argmin_{\pi\in\Pi_\alpha(\mu,\nu)} \langle C ,\pi \rangle.
    \label{eq:optimal_pi_reg1}
\end{equation}
Fig.~\ref{fig:polyotope2} shows an pictorial representation of this formulation.

\begin{figure}
    \centering
    \includegraphics[width=0.4\textwidth]{img/week5_polytope2.pdf}
    \caption{Illustration of the transport polytope $\Pi(\mu,\nu)$, the defined subset $\Pi_\alpha(\mu,\nu)$ and their respective optimal plans.}
    \label{fig:polyotope2}
\end{figure}

Given that the optimisation variable in this setting is $\pi$, we can write 
\begin{align}
    \KL{\pi}{\mu\otimes\nu} 
    &= \sum_{i=1}^{N}\sum_{j=1}^{M} \pi_{i,j}\log\frac{\pi_{i,j}}{\mu_i\nu_j} \nonumber \\
    &\propto_\pi  \sum_{i=1}^{N}\sum_{j=1}^{M} \pi_{i,j}\log \pi_{i,j} \nonumber\\
    &=  -H(\pi),
\end{align}
where $\propto_\pi$ denotes equality up to additive terms independent of $\pi$
(since the marginals of $\pi$ are fixed to be $\mu$ and $\nu$).

As a consequence, the optimal plan in eq.~\eqref{eq:optimal_pi_reg1} can be rewritten using a Lagrangian formulation as 
\begin{equation}
    \pi_\alpha^\star = \argmin_{\pi\in\Pi(\mu,\nu)} \langle C ,\pi \rangle - \frac{1}{\lambda} H(\pi),
    \label{eq:optimal_pi_reg2}
\end{equation}
where there is one, and only one, $\alpha\in\R_+$ for each $\lambda\in\R_+$.

The Lagrangian is given by 
\[
\mathcal{L}(\pi,f,g)
=
\sum_{i=1}^{n}\sum_{j=1}^{m}\left(\frac{1}{\lambda}\pi_{ij} \log\pi_{ij} + \pi_{ij}c_{ij}\right)
+ f^\top(\pi\mathbf{1}_m - \mu) + g^\top(\pi^\top\mathbf{1}_n - \nu),
\]
and thus $\partial \mathcal{L} / \partial \pi_{ij} = 0$ yields
\[
\pi_{ij} = e^{-1-\lambda f_i}\,e^{-\lambda c_{ij}}\,e^{-\lambda g_j},
\]
meaning that the solution has the form
\[
\pi_\lambda^* = \diag(u)\, K \,\diag(v),
\]
where $K_{ij}= \exp\!\left(-\lambda C_{ij} \right)$ (entrywise). \\

Finding $u$ and $v$ is not easy. However, \cite{sinkhorn_1964} states that since the entries of $K$ are positive, there exist (essentially unique) scaling vectors $u,v$ such that $\diag(u) K \diag(v)\in\Pi({\mu},{\nu})$. Therefore, $u,v$ can be updated so that $\diag(u) K \diag(v)$ satisfies the marginal constraints, that is (Sinkhorn iterations),
\begin{equation}
    u \leftarrow \mu./(Kv), \qquad v \leftarrow \nu./(K^\top u ).
\end{equation}

Fig.~\ref{fig:sinkhorn_iterations} shows an illustration of optimal transport plans found via linear programming and Sinkhorn.

\begin{figure}
    \centering
    \includegraphics[width=0.45\textwidth]{img/week5_EMD_vs_S1.pdf}
    \hfill
    \includegraphics[width=0.45\textwidth]{img/week5_EMD_vs_S2.pdf}   
    \caption{Optimal transport plans between 2-component Gaussian mixtures: linear programming (left) and Sinkhorn iterations (right).}
    \label{fig:sinkhorn_iterations}
\end{figure}



\section{Metric properties}

The critical feature of OT theory in the context of generative modelling is the OT problem defines a distance between probability distributions. For this, it is necessary that the cost function takes the form $c(x,y) = d(x,y)^p$, where $d(\cdot,\cdot)$ is a distance defined on the support of the distributions involved, and $p\in\N$. Critically, OT is said to \emph{lift} the distance $d$ from the sample space to the space of distributions, thus translating the geometric properties from the sample to the distribution space. 

\begin{definition}[Wasserstein distance]
Let $(\cX,d)$ be a metric space and let $\mu$ and $\nu$ be probability measures on
$\cX$ with finite $p$-th moments. The \emph{$p$-Wasserstein distance} between $\mu$
and $\nu$ is defined as
\[
W_p(\mu,\nu)
\defeq
\left(
\inf_{\pi\in\Pi(\mu,\nu)}
\int_{\cX\times\cX} d(x,y)^p \, d\pi(x,y)
\right)^{1/p}.
\]
\end{definition}

\begin{remark}[Metric properties of the Wasserstein distance]
For $p\ge 1$, the $p$-Wasserstein distance $W_p$ defines a true metric on the space
of probability measures with finite $p$-th moments: it is nonnegative, symmetric,
vanishes if and only if the two measures coincide, and satisfies the triangle
inequality. The first three properties follow directly from the definition, while
the triangle inequality relies on the ability to \emph{glue} optimal transport
plans. Intuitively, transporting mass from $\mu$ to $\nu$ and then from $\nu$ to
$\lambda$ cannot be cheaper than transporting it directly from $\mu$ to
$\lambda$: since one optimal plan defines the lowest-cost transport procedure, forcing the transport to visit any intermediate measure can only increase the overall cost.
\end{remark}


\begin{remark}[Choice of the ground metric]
When $\cX\subseteq\R^d$, we will mostly consider 
$d(x,y)=\|x-y\|_2$, the Euclidean distance. This choice leads to a particularly rich geometric theory and allows for explicit solutions and efficient algorithms in a number of cases, such as Gaussian measures and one-dimensional distributions as seen above.
\end{remark}

\begin{remark}[Discrete measures]
When $\mu=\sum_{i=1}^N \mu_i\delta_{x_i}$ and $\nu=\sum_{j=1}^M \nu_j\delta_{y_j}$ are discrete probability measures, the $p$-Wasserstein distance reduces to a finite dimensional optimization problem:
\[
W_p^p(\mu,\nu)
=
\min_{P\in\Pi(\mu,\nu)}
\sum_{i=1}^N\sum_{j=1}^M d(x_i,y_j)^p\,P_{ij},
\]
where $\Pi(\mu,\nu)$ denotes the transport polytope. Recall that, in this case, computing $W_p$ amounts to solving a linear program.
\end{remark}

Fig.~\ref{fig:Wasserstein_1d} shows a comparison between $p$-Wasserstein and $l_p$ distances for distributions as their support becomes far from one another. 
\begin{figure}
    \centering
    \includegraphics[width=0.9\textwidth]{img/week5_Wasserstein_1d.pdf}
    \caption{Comparison between $W_p$ and $l_p$, $p\in\{1,2\}$, for two unimodal distributions. Notice how the $l_p$ distances become uninformative as the support between the source and target distributions becomes too far from each other; conversely, the $W_p$ distances inherit the geometric property of the ground distance and keep an account of such separation.}
    \label{fig:Wasserstein_1d}
\end{figure}

\missing{Add the missing axis labels in Fig.~\ref{fig:Wasserstein_1d}. }

\begin{remark}[Suitability of $W_p$ for learning]
Thus far, we have considered the Wasserstein distance as a metric on spaces of
probability measures. In learning applications, however, we are typically
interested in spaces of \emph{generative models}, that is, families of probability
distributions $(P_\theta)_{\theta\in\Theta}$ indexed by parameters $\theta$. In
this setting, learning requires not only a notion of distance, but also a
meaningful notion of convergence.

Let $\mu_{\text{data}}$ denote the true data distribution. The goal of learning is
to find parameters $\theta$ such that $P_\theta$ converges to $\mu_{\text{data}}$,
or equivalently, such that $D(\mu_{\text{data}},P_\theta)\to 0$ for a reasonable
divergence $D$. A key advantage of Wasserstein distances is that they metrize a
notion of convergence that is sensitive to the geometry of the underlying space.
For instance, if $x_i\to x_0$ in $\mathbb{R}^d$, then $W_p(\delta_{x_i},\delta_{x_0})
\to 0$, whereas many classical divergences fail to capture this intuitive
convergence. As a result, the Wasserstein distance provides a natural framework both for constructing sequences of generative models that converge to a target distribution and assess that convergence, that is, to design a learning algorithm.


\end{remark}


\paragraph{Geodesics in the Wasserstein space.}

\begin{definition}[Wasserstein space]
Let $(\cX,d)$ be a metric space and let $p\ge 1$. Denote by
$\cP(\cX)$ the set of all probability measures on $\mathcal X$.
The \emph{$p$-Wasserstein space} is defined as
\[
\mathcal W_p(\mathcal X)
=
\left\{
\mu \in \cP(\mathcal X)
\;\middle|\;
\int_{\cX} d(x,x_0)^p\,d\mu(x) < \infty
\ \text{for some (hence any) } x_0\in\mathcal X
\right\},
\]
and is equipped with the $p$-Wasserstein distance $W_p$.
\end{definition}

\begin{remark}[Intuition]
The Wasserstein space $\mathcal W_p(\cX)$ consists of all probability
measures whose mass does not escape to infinity too quickly, so that transporting
mass over distance according to the cost $d^p$ has finite total cost.
\end{remark}


In geometry, a geodesic generalises the notion of a straight line: it is the shortest path between two points in a given space. In the Euclidean space equipped with the $\ell_2$ distance, geodesics correspond to linear interpolations between points, and the space is said to be geodesic. A similar notion exists in the Wasserstein space. While a naive interpolation between distributions, given by $(1-t)\mu_1 + t\mu_2$, corresponds to a \emph{vertical} interpolation of densities---see Fig.~\ref{fig:Euclidean_geodesic}, it does not generally represent the shortest path in Wasserstein space.

\begin{figure}[h]
    \centering
    \includegraphics[trim={2cm 4.5cm 2cm 5cm},clip, width=0.55\textwidth]{img/week5_straightLine.pdf}
    \hfill
    \includegraphics[trim={2cm 2cm 2cm 2cm},clip, width=0.35\textwidth]{img/week5_geodesic_Euc_1d.pdf}
    \caption{Left: geodesics as straight lines in Euclidean space. Right: linear
    interpolation between one-dimensional distributions.}
    \label{fig:Euclidean_geodesic}
\end{figure}

The Wasserstein space $\mathcal{W}_p$ is itself a geodesic space. When an optimal transport map $T$ exists between two measures $\mu_1$ and $\mu_2$, a Wasserstein geodesic connecting them is given by
\[
\forall t\in[0,1], \qquad
\mu^{1\to 2}(t) = \bigl((1-t)\mathrm{Id} + tT\bigr)_\# \mu_1.
\]
This construction represents a \emph{horizontal} interpolation of mass, where probability is gradually displaced along optimal transport paths. As a result, Wasserstein geodesics provide intuitive and physically meaningful interpolations between distributions, as illustrated in Fig.~\ref{fig:W_geodesic_1d}.

\begin{figure}[h]
    \centering
    \includegraphics[trim={2cm 2cm 2cm 2cm},clip, width=0.5\textwidth]{img/week5_geodesic_1d.pdf}
    \caption{A Wasserstein geodesic between two one-dimensional distributions,
    obtained by transporting mass along optimal paths.}
    \label{fig:W_geodesic_1d}
\end{figure}

\paragraph{Wasserstein barycenters.}
As direct consequence of the Geodesic property of the Wasserstein space,  optimal transport also provides a natural notion of averaging multiple probability measures. Given a collection of distributions $(\mu_i)_{i=1}^s$ and weights $\lambda_i>0$ such that $\sum_{i=1}^s \lambda_i=1$, the \emph{Wasserstein barycenter} is defined as
\[
\bar{\mu}
=
\arg\min_{\mu}
\sum_{i=1}^s \lambda_i\, W_p^p(\mu,\mu_i).
\]
This definition generalizes Euclidean averaging to the space of probability
measures, taking into account the geometry induced by optimal transport. For discrete measures, one may restrict the optimization to measures with fixed support, fixed weights, or both, leading to tractable computational formulations.

\begin{figure}[h]
    \centering
    \includegraphics[trim={2cm 2cm 2cm 2cm},clip, width=0.45\textwidth]{img/week5_distrib_bary.pdf}
    \hfill
    \vline
    \hfill
    \includegraphics[trim={2cm 2cm 2cm 2cm},clip, width=0.45\textwidth]{img/week5_barycenter_2d.pdf}
    \caption{Illustration of Wasserstein barycenters: averaging multiple
    distributions while respecting the geometry of the underlying space.}
\end{figure}

Wasserstein barycenters differ fundamentally from Euclidean averages, especially in applications such as image processing. While Euclidean averaging often leads to blurred or unrealistic results, Wasserstein barycenters preserve geometric structure by transporting mass coherently across samples. This contrast is illustrated in Fig.~\ref{fig:bary_images}, where averaging images in Wasserstein
space produces sharper and more meaningful averages.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.48\textwidth]{img/week5_symboles_wine_L2.pdf}
    \hfill
    \includegraphics[width=0.48\textwidth]{img/week5_symboles_wine_bary.pdf}
    \caption{Left: Euclidean averaging of images. Right: Wasserstein barycenter,
    which preserves geometric features.}
    \label{fig:bary_images}
\end{figure}

%\section{Application: colour transfer}
\clearpage
\newpage
\section*{Exercises - Optimal Transport}

\begin{exercise}[Monge existence/non-existence]
Give two concrete examples with \(N=3\), \(M=3\), and different weight vectors  \(\mu,\nu\) with strictly positive coordinates 
such that i) a Monge map \(T\) satisfying \(T_\#\mu=\nu\) exists, and ii) a Monge map \(T\) satisfying \(T_\#\mu=\nu\) cannot exist. Explain briefly why the problem in the second case is the impossibility of mass splitting.
\end{exercise}

\begin{solution}[Monge existence/non-existence]

Let $\mu=(\mu_1,\mu_2,\mu_3)$ on $\{x_1,x_2,x_3\}$ and 
$\nu=(\nu_1,\nu_2,\nu_3)$ on $\{y_1,y_2,y_3\}$, all coordinates strictly positive.

\medskip
\noindent\textbf{i) Existence.}
Take 
\[
\mu=(1/2,\,1/3,\,1/6),
\qquad
\nu=(1/3,\,1/6,\,1/2).
\]
Define the map
\[
T(x_1)=y_3, 
\qquad 
T(x_2)=y_1, 
\qquad 
T(x_3)=y_2.
\]
Then $T_\#\mu=\nu$, since each target mass equals exactly one source mass.

\medskip
\noindent\textbf{ii) Non-existence.}
Take 
\[
\mu=(1/2,\,1/4,\,1/4),
\qquad
\nu=(1/3,\,1/3,\,1/3).
\]
For any map $T:\{x_1,x_2,x_3\}\to\{y_1,y_2,y_3\}$, we have
\[
(T_\#\mu)(y_j)
=
\sum_{i:\,T(x_i)=y_j}\mu_i,
\]
so target mass value should be of the form $\sum_{a\in A} a$, where $A$ is a subset of 
\[
\cA = \{1/2,1/4,1/4\}.
\]
Equivalently, the target values are in the set
\[
\{0,\;1/4,\;1/2,\;3/4,\;1\}.
\]
Since $1/3$ does not belong to this set, no such $T$ can satisfy $T_\#\mu=\nu$.

The non-existence of a map in the second case is that a Monge map cannot \emph{split} mass: each source atom must be sent entirely to a single target point, so target masses must be sums of whole source masses rather than arbitrary fractions.
\end{solution}



\begin{exercise}[Monge non-uniqueness]
Construct a discrete example with \(N=M=2\) and a cost matrix \(C\) with strictly-positive entries such that there are at
least two distinct Monge maps achieving the same optimal cost.
\end{exercise}

\begin{solution}[Monge non-uniqueness]
Let $\mu=\nu=(1/2,\,1/2)$ on $\{x_1,x_2\}$ and $\{y_1,y_2\}$, and take the strictly-positive cost matrix
\[
C=\begin{pmatrix}
1 & 1\\
1 & 1
\end{pmatrix}.
\]
Then any Monge map has total cost
\[
\frac12\,C_{1,T(x_1)}+\frac12\,C_{2,T(x_2)}=\frac12\cdot 1+\frac12\cdot 1=1.
\]
In particular, the two distinct Monge maps
\[
T_1(x_1)=y_1,\quad T_1(x_2)=y_2,
\qquad\text{and}\qquad
T_2(x_1)=y_2,\quad T_2(x_2)=y_1
\]
both satisfy $T_\#\mu=\nu$ and achieve the same (optimal) cost.
\end{solution}


\begin{exercise}[A coupling always exists]
Let \(\mu\in\mathbb{R}^N_+\) and \(\nu\in\mathbb{R}^M_+\) with \(\sum_i\mu_i=\sum_j\nu_j=1\).
Show that \(P=\mu\nu^\top\) belongs to \(\Pi(\mu,\nu)\). Interpret this plan as an
\emph{independent coupling} and explain how mass is transported under this plan.
\end{exercise}

\begin{solution}[A coupling always exists]
Let $P=\mu\nu^\top$, i.e.\ $P_{ij}=\mu_i\nu_j\ge 0$. Then the row sums are
\[
\sum_{j=1}^M P_{ij}=\sum_{j=1}^M \mu_i\nu_j=\mu_i\sum_{j=1}^M \nu_j=\mu_i,
\]
and the column sums are
\[
\sum_{i=1}^N P_{ij}=\sum_{i=1}^N \mu_i\nu_j=\nu_j\sum_{i=1}^N \mu_i=\nu_j.
\]
Hence $P\mathbf 1_M=\mu$ and $P^\top \mathbf 1_N=\nu$, so $P\in\Pi(\mu,\nu)$.

This plan is called the \emph{independent coupling} because it corresponds to drawing
$(X,Y)$ with $X\sim\mu$ and $Y\sim\nu$ independently, i.e.\ $\mathbb P(X=x_i,Y=y_j)=\mu_i\nu_j$.
Under this plan, each source mass $\mu_i$ is split proportionally across all targets:
from $x_i$ (where there is a $\mu_i$ amount of total mass) an amount $\mu_i\nu_j$ is sent to $y_j$ for every $j$.
\end{solution}


\begin{exercise}[Solve a $2\times 2$ OT problem by hand]
Let \(N=M=2\), \(\mu=(\mu_1,\mu_2)\), \(\nu=(\nu_1,\nu_2)\), where $\mu_1+\mu_2=\nu_1+\nu_2=1$, and
\[
C=\begin{pmatrix}
c_{11} & c_{12}\\
c_{21} & c_{22}
\end{pmatrix}.
\]
Solve the Kantorovich problem: find an optimal plan \(P^\star\in\Pi(\mu,\nu)\) and the
optimal cost.
\end{exercise}

\begin{solution}[Solve a $2\times 2$ OT problem by hand]
Any $P\in\Pi(\mu,\nu)$ is determined by $p:=P_{11}$:
\[
P(p)=
\begin{pmatrix}
p & \mu_1-p\\
\nu_1-p & \mu_2-\nu_1+p
\end{pmatrix},
\qquad 
p\in\big[\max\{0,\nu_1-\mu_2\},\ \min\{\mu_1,\nu_1\}\big].
\]
The cost is affine in $p$:
\begin{align*}
\langle C,P(p)\rangle
&= c_{11}p+c_{12}(\mu_1-p)+c_{21}(\nu_1-p)+c_{22}(\mu_2-\nu_1+p)\\
&= \text{const} \;+\; p\,\Delta,
\end{align*}
where
\[
\Delta := c_{11}-c_{12}-c_{21}+c_{22}.
\]
Hence the optimum is attained at an extreme of the feasible region:
\[
p^\star=
\begin{cases}
\max\{0,\nu_1-\mu_2\}, & \Delta>0,\\[2pt]
\min\{\mu_1,\nu_1\}, & \Delta<0,\\[2pt]
\text{any feasible }p, & \Delta=0,
\end{cases}
\qquad
P^\star=P(p^\star),
\qquad
\text{opt.\ cost }=\langle C,P(p^\star)\rangle.
\]

\end{solution}




\begin{exercise}[1D OT between a two-point distribution and a uniform law]
Let \(\mu=\tfrac12\delta_{0}+\tfrac12\delta_{1}\) and \(\nu=\mathrm{Unif}([0,1])\) on \(\mathbb{R}\).
Using the quantile representation, compute \(W_1(\mu,\nu)\), i.e., assume a cost $c(x,y) = |x-y|$, and describe what the optimal plan does
(compute the integrals explicitly).
\end{exercise}

\begin{solution}[1D OT between a two-point distribution and a uniform law]
In one dimension,
\[
W_1(\mu,\nu)=\int_0^1 \big|F_\mu^{-1}(t)-F_\nu^{-1}(t)\big|\,dt.
\]
Here $F_\nu^{-1}(t)=t$ for $t\in[0,1]$, and
\[
F_\mu^{-1}(t)=
\begin{cases}
0, & t\in[0,1/2],\\
1, & t\in(1/2,1].
\end{cases}
\]
Therefore
\begin{align*}
W_1(\mu,\nu)
&=\int_0^{1/2} |0-t|\,dt+\int_{1/2}^{1} |1-t|\,dt \\
&=\int_0^{1/2} t\,dt+\int_{1/2}^{1} (1-t)\,dt
=\frac{1}{8}+\frac{1}{8}=\frac14.
\end{align*}

The optimal coupling is the monotone (quantile) coupling: it sends the lower half of the uniform mass
$t\in[0,1/2]$ to $0$ and the upper half $t\in(1/2,1]$ to $1$, i.e.\ it splits $\nu$ into two halves and
matches them with the two atoms of $\mu$.\\
\textbf{Proposed:} Repeat the exercise for $W_2$.
\end{solution}


\begin{exercise}[Wasserstein geodesic between two Gaussians in one dimension]
Let \(\mu_0=\mathcal N(m_0,\sigma_0^2)\) and \(\mu_1=\mathcal N(m_1,\sigma_1^2)\) on \(\mathbb{R}\).
Using the fact that the optimal map is affine for any $p\geq 1$, show that the displacement interpolation
\(\mu_t=((1-t)\mathrm{Id}+tT)_\#\mu_0\) is Gaussian, and identify its mean and variance as functions of \(t\).
\end{exercise}

\begin{solution}[Wasserstein geodesic between two Gaussians in one dimension]
In 1D the optimal map from $\mu_0=\mathcal N(m_0,\sigma_0^2)$ to $\mu_1=\mathcal N(m_1,\sigma_1^2)$ is affine:
\[
T(x)=m_1+\frac{\sigma_1}{\sigma_0}(x-m_0).
\]
Define the interpolation map
\[
S_t(x):=(1-t)x+tT(x)
= \Big((1-t)+t\frac{\sigma_1}{\sigma_0}\Big)x
+ t\Big(m_1-\frac{\sigma_1}{\sigma_0}m_0\Big),
\qquad t\in[0,1].
\]
Since $S_t$ is affine and $X\sim\mathcal N(m_0,\sigma_0^2)$ implies $S_t(X)$ is Gaussian, we have
\[
\mu_t=(S_t)_\#\mu_0=\mathcal N(m_t,\sigma_t^2),
\]
with mean and variance
\[
m_t = (1-t)m_0+t m_1,
\qquad
\sigma_t = (1-t)\sigma_0+t\sigma_1,
\qquad
\sigma_t^2 = \big((1-t)\sigma_0+t\sigma_1\big)^2.
\]
\end{solution}



\begin{exercise}[Gaussian OT in one dimension]
Let \(\mu=\mathcal{N}(m_\mu,\sigma_\mu^2)\) and \(\nu=\mathcal{N}(m_\nu,\sigma_\nu^2)\).
\begin{enumerate}
\item Derive the optimal transport map \(T(x)=m_\nu+\frac{\sigma_\nu}{\sigma_\mu}(x-m_\mu)\).
\item Compute \(W_2^2(\mu,\nu)\) and simplify the expression.
\end{enumerate}
\end{exercise}

\begin{solution}[Gaussian OT in one dimension]
\textbf{1)} In one dimension (for any $p$), the optimal transport map is the monotone rearrangement
\[
T = F_\nu^{-1}\circ F_\mu.
\]
For $\mu=\mathcal N(m_\mu,\sigma_\mu^2)$,
\[
F_\mu(x)=\Phi\!\left(\frac{x-m_\mu}{\sigma_\mu}\right),
\]
where
\[
\Phi(t)=\frac{1}{\sqrt{2\pi}}\int_{-\infty}^t e^{-s^2/2}\,ds
\]
denotes the cumulative distribution function of the standard normal distribution.
Similarly, for $\nu=\mathcal N(m_\nu,\sigma_\nu^2)$,
\[
F_\nu^{-1}(u)=m_\nu+\sigma_\nu\,\Phi^{-1}(u).
\]
Hence
\[
T(x)=F_\nu^{-1}(F_\mu(x))
= m_\nu+\sigma_\nu\,\Phi^{-1}\!\left(\Phi\!\left(\frac{x-m_\mu}{\sigma_\mu}\right)\right)
= m_\nu+\frac{\sigma_\nu}{\sigma_\mu}(x-m_\mu).
\]

\textbf{2)} Let $X\sim\mu$ and set $Y=T(X)$, so $Y\sim\nu$ and
\[
W_2^2(\mu,\nu)=\mathbb E\big[(X-Y)^2\big]=\mathbb E\big[(X-T(X))^2\big].
\]
Write $X=m_\mu+\sigma_\mu Z$ with $Z\sim\mathcal N(0,1)$. Then $T(X)=m_\nu+\sigma_\nu Z$, so
\[
X-T(X)=(m_\mu-m_\nu)+(\sigma_\mu-\sigma_\nu)Z,
\]
and using $\mathbb E[Z]=0$, $\mathbb E[Z^2]=1$,
\[
W_2^2(\mu,\nu)=(m_\mu-m_\nu)^2+(\sigma_\mu-\sigma_\nu)^2.
\]
\textbf{Proposed:} How does the solution change for other non-Gaussian location-scale distributions (in 1d)?
\end{solution}





\begin{exercise}[1D Wasserstein barycenter of two distributions]
Let $\mu_1=\mathrm{Unif}([0,1])$ and $\mu_2=\mathrm{Unif}([2,3])$ on $\mathbb{R}$, and let
$\lambda\in(0,1)$.
Compute explicitly the $2$-Wasserstein barycenter
\[
\bar\mu=\arg\min_\mu \ \lambda W_2^2(\mu,\mu_1)+(1-\lambda)W_2^2(\mu,\mu_2).
\]
Identify the support of $\bar\mu$ and describe it geometrically.
\end{exercise}


\begin{solution}[1D Wasserstein barycenter of two distributions]
In 1D, the $W_2$-barycenter has quantile function given by the convex combination of quantiles:
\[
F_{\bar\mu}^{-1}(t)=\lambda F_{\mu_1}^{-1}(t)+(1-\lambda)F_{\mu_2}^{-1}(t),\qquad t\in(0,1).
\]
Since $F_{\mu_1}^{-1}(t)=t$ and $F_{\mu_2}^{-1}(t)=2+t$, we have
\[
F_{\bar\mu}^{-1}(t)=\lambda t+(1-\lambda)(2+t)=t+2(1-\lambda).
\]
Therefore $\bar\mu=\mathrm{Unif}([2(1-\lambda),\,1+2(1-\lambda)])=\mathrm{Unif}([2-2\lambda,\,3-2\lambda])$.

The barycenter is supported on the interval $[2-2\lambda,\,3-2\lambda]$, i.e.\ the unit-length interval obtained by translating
$[2,3]$ left by $2\lambda$ (equivalently, translating $[0,1]$ right by $2(1-\lambda)$). Geometrically, the barycenter
slides linearly between the two intervals.
\end{solution}




\begin{exercise}[Barycenter vs Euclidean averaging]
Let $\mu_1=\delta_0$ and $\mu_2=\delta_1$ on $\mathbb{R}$.
\begin{enumerate}
\item Compute the Euclidean average $\tfrac12(\mu_1+\mu_2)$.
\item Compute the Wasserstein barycenter with equal weights, i.e., $\lambda = 1/2$.
\end{enumerate}
Compare and interpret the two results.
\end{exercise}


\begin{solution}[Barycenter vs Euclidean averaging]
1) The Euclidean (mixture) average is
\[
\tfrac12(\mu_1+\mu_2)=\tfrac12\delta_0+\tfrac12\delta_1.
\]

2) The $W_2$-barycenter in 1D with equal weights has quantile
\[
F_{\bar\mu}^{-1}(t)=\tfrac12 F_{\mu_1}^{-1}(t)+\tfrac12 F_{\mu_2}^{-1}(t).
\]
Since $F_{\delta_0}^{-1}(t)=0$ and $F_{\delta_1}^{-1}(t)=1$ for all $t\in(0,1)$, we get
\[
F_{\bar\mu}^{-1}(t)=\tfrac12,
\]
hence $\bar\mu=\delta_{1/2}$.

The Euclidean average is a \emph{mixture} with mass split between $0$ and $1$,
whereas the Wasserstein barycenter \emph{moves} the mass to the midpoint, producing a single Dirac at $1/2$.\\
\textbf{Proposed:} Repeat the exercise for $\mu_0 = \cN(0,\sigma^2)$ and $\mu_1 = \cN(1,\sigma^2)$.
\end{solution}
