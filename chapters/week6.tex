\chapter{Deep Latent Variable Models}

\textbf{NB:} This is based on \cite{kingma2013auto}.

\section{Motivation}


\section{A crash course on neural networks}

For completion, we will revisit NNs. 

\begin{itemize}
    \item feedforward networks
    \item brief metion o other architectures
    \item Back prop
    \item autoencoders
\end{itemize}

\section{Stochastic gradient variational Bayes}

Let us revisit the latent-variable formulation that we have been assuming throughput the module. Consider a dataset $\x = \{x_1,\ldots,x_N\}$ of i.i.d.~samples generated by an unknown generative model with unknown density $p_\theta(x)$. Furthermore, assume that the data generation involves a latent variable $z$, meaning that the generation process consists of sampling $z_n\sim p_\theta(z)$, and then $x_n\sim p_\theta(x \mid z_n)$. 

We will assume standard regularity conditions, such as differentiability of all the involved densities wrt their arguments and the parameters $\theta$; also note that $\theta$ contains the parameters of all the densities: the prior $p_\theta(z)$ and the likelihood $p_\theta(x \mid z)$. Furthermore, unlike previous sections in the module, we do not make any structural assumptions of the model, in particular, we do not assume the posterior $p_\theta(z \mid x)$ is tractable (as in EM) or that the involved densities are conjugate such as in the mixture models we have seen before.  

Using this very general setup, our goal is threefold:
\begin{enumerate}
    \item to obtain an efficient maximum likelihood estimator for $\theta$;
    \item to construct an efficient approximation of the posterior distribution $p_\theta(z \mid x)$;
    \item to approximate the marginal distribution $p_\theta(x)$ in a way that allows us to synthesise new data directly.
\end{enumerate}

These objectives are non-trivial. In particular, we must design methods that are robust to potential intractabilities arising from specific choices of the model densities. Moreover, the models of interest are typically complex and high-dimensional, and can only be trained effectively in the large-data regime.


We will follow the standard VI approach seen in the previous lectures, that is, we will consider the variational approximation of the posterior $p_\theta(z \mid x)$ denoted $q_\phi(z \mid x)$, which will be optimised by optimising the ELBO. 

For a single datapoint\footnote{Just like the evidence, the ELBO is additive, i.e., $\elbo(\theta,\phi,x) = \sum_{n=1}^{N} \elbo(\theta,\phi,x_n)$} $x_n$, the ELBO is given by 
\begin{equation}
    \elbo(\theta,\phi,x_n) = -\KL{q_\theta(z \mid x_n)}{p_\theta(z)} + \mathbb{E}_{q_\theta(z \mid x_n)}\left[\log p_\theta(x_n \mid z)\right].
    \label{eq:ELBO_single_datapoint}
\end{equation}

\begin{remark}[Autoencoder arquitecture]
    Notice that the general setting defined so far allows for flexible models as we have not assumed any form for the model's likelihood $p_\theta(x_n \mid z)$ and prior $p_\theta(z)$; and the variational approximation $q_\theta(z \mid x_n)$. We are therefore free to choose expressive models for these densities optimised wrt the ELBO in eq.~\eqref{eq:ELBO_single_datapoint}. Observe that this optimisation procedure implies an autoencoding architecture: for a given choice of the prior  $p_\theta(z)$, optimising the ELBO gives i) an encoder model $q_\theta(z \mid x_n)$ that maps the data to the latent space, and then ii) a decoder $p_\theta(x_n \mid z)$ that maps the latent code to the ambient space. The model will then be specified by adopting flexible parametrisations of these densities using neural networks. 
\end{remark}

For completeness, we define the following objects. 

\begin{definition}
    We will refer to $q_\theta(z \mid x_n)$ and $p_\theta(x_n \mid z)$ as the (probabilistic) encoder and decoder respectively.
\end{definition}

\paragraph{Sample approximation of the ELBO.}


We are now interested in maximising the ELBO, and we will approach that problem using gradient-based optimisation. Since the ELBO---including the KL term in eq.~\eqref{eq:ELBO_single_datapoint}---can be expressed  as the expectation of $f:\cZ\to\R$ given by 
\begin{equation}
    f(z) = - \log\frac{q_\theta(z \mid x_n)}{p_\theta(z)} + \log p_\theta(x_n \mid z)
\end{equation}
wrt $q_\theta(z \mid x_n)$, recall that its gradients can be approximated using Monte Carlo samples as
\begin{equation}
    \label{eq:MC_ELBO}
    \nabla_\phi \mathbb{E}_{q_\theta(z \mid x_n)}\left[f(z)\right] 
    = \mathbb{E}_{q_\theta(z \mid x_n)}\left[f(z) \nabla_\phi \log q_\theta(z \mid x_n) \right]
    \simeq \frac{1}{L} \sum_{l=1}^{L} f(z^{(l)}) \nabla_\phi \log q_\theta(z^{(l)} \mid x_n),
\end{equation}
where $z^{(l)}\sim q_\theta(z \mid x_n)$. However, it has been shown that this estimator exhibits a large variance and is us this unsuitable for our setting.

To introduce a practical estimator of the ELBO and its gradients, we adopt two main considerations that will allow us to calculate each term in eq.~\eqref{eq:ELBO_single_datapoint}. For the first term (the KL divergence), recall that the general setting defined so far allows for flexible models for both the prior over latent $p_\theta(z)$ and also the likelihood $p_\theta(x \mid z)$. Since the latent variable in this formulation is not tied to have any particular physical interpretation, this generality might be seen as a redundant modelling choice: considering a simple model for the prior $p_\theta(z)$ should not be a limitation to design an expressive model $p_\theta(x)$, since the required flexibility can be represented through the likelihood $p_\theta(x \mid z)$. As a consequence, we will assume we are free to chose a simple---and thus interpretable, easy to sample---prior $p_\theta(z)$. 

Given such a choice for the prior of the latent variable, the choice of the variational approximation $q_\phi(z \mid x_n)$ can also be done with tractability of the KL term in mind. Note that $q_\phi(z \mid x_n)$ is not a component of the generative model, but rather of the approximation. 

\begin{remark}
    The consideration above suggests that we should choose both $p_\theta(z)$ and $q_\phi(z \mid x_n)$  such that the KL between them is tractable. Note that this does not restrict the modelling generality of the decoder model, which can be chosen to have a tractable form, e.g., Gaussian or Bernoulli, with parameters controlled by a neural network.
\end{remark}

A second consideration is pertains the second term of eq.~\eqref{eq:ELBO_single_datapoint}, referred to as the \emph{expected reconstruction error} given by 
\begin{equation}
    \mathbb{E}_{q_\theta(z \mid x_n)}\left[\log p_\theta(x_n \mid z)\right].
\end{equation}
Rather than performing a Monte Carlo approximation of this term by sampling directly from the decoder $z^{(l)}\sim q_\theta(z \mid x_n)$ as stated above, we will use the reparametrisation trick. That is, we will assume that the sample $z^{(l)}$ can be expressed as s deterministic transformation of an auxiliary variable $\epsilon\in\R,\,\sim p(\epsilon)$ through the function
\begin{align}
    g_\phi: \R \times \cX &\to \cZ\\
    (\epsilon,x) &\mapsto z = g_\phi(\epsilon,x).    
\end{align}

With this change of variables, the expectation under the variational distribution can be expressed as 

\begin{equation}
    \mathbb{E}_{q_\theta(z \mid x_n)}\left[f(z)\right] = \mathbb{E}_{p(\epsilon)}\left[f(z)\right],
\end{equation}
where $ z = g_\phi(\epsilon,x)$. 

    Since the expectation no longer depend on the variational parameters as in eq.~\eqref{eq:MC_ELBO}, the gradient and expectation operators can be interchanged, and thus we can produce a simple Monte Carlo estimate of the gradient given by 
    \begin{align}
        \nabla_\phi \mathbb{E}_{q_\theta(z \mid x_n)}\left[f(z)\right] 
        & = \nabla_\phi\mathbb{E}_{p(\epsilon)}\left[f(z)\right]\nonumber\\
        & = \mathbb{E}_{p(\epsilon)}\left[ \nabla_\phi f(z)\right]\nonumber\\
        & \simeq \frac{1}{L}\sum_{l=1}^{L}f(z^{(l)}).
    \end{align}

\begin{remark}
The introduction of the reparameterisation trick in this context has several key advantages. 
First, the resulting gradient estimator is unbiased. 
Second, it typically has significantly lower variance than estimators that differentiate through the sampling distribution itself, because the randomness is isolated in an auxiliary variable and gradients are taken through a deterministic mapping---see Fig.~\ref{fig:rep_trick}. 
Third, by disentangling the source of randomness from the variational parameters, the objective becomes directly differentiable with respect to $\phi$, allowing us to use standard automatic differentiation and backpropagation.
\end{remark}


\begin{figure}
    \centering
    \includegraphics[
  width=0.75\textwidth,
  trim=2.5cm 10cm 2.5cm 6.5cm,
  clip
]{img/week6_rep_trick_book.pdf}
    \caption{Illustration of the reparapetrisation trick. Taken from \protect\cite{kingma2019introductionvae}}
    \label{fig:rep_trick}
\end{figure}

\begin{remark}
A natural question concerns the choice of the mapping $g_\phi$. 
By construction, $g_\phi$ must define a pushforward of a simple base distribution:
\[
z = g_\phi(x,\epsilon), 
\qquad \epsilon \sim p(\epsilon),
\]
so that the induced distribution of $z$ matches the variational family $q_\phi(z \mid x)$. 
In other words, $q_\phi(\cdot \mid x)$ is the pushforward of $p(\epsilon)$ under $g_\phi(x,\cdot)$.

This leaves two modelling choices: the base distribution $p(\epsilon)$ and the transformation $g_\phi$. 
In practice, $p(\epsilon)$ is typically chosen to be simple (e.g.\ a standard Gaussian), and modelling flexibility is introduced through the parameterised map $g_\phi$. 
For example, if $q_\phi(z \mid x)$ is Gaussian with mean $\mu_\phi(x)$ and diagonal covariance $\Sigma_\phi(x)$, one may take
\[
g_\phi(x,\epsilon) = \mu_\phi(x) + \Sigma_\phi(x)^{1/2} \epsilon.
\]
More expressive variational families can be obtained by choosing richer transformations $g_\phi$, as in normalising flows.

Thus, in most applications, we fix $p(\epsilon)$ to be simple and design $g_\phi$ so that it is sufficiently expressive while remaining differentiable and computationally tractable.
\end{remark}

\section{Variational Autoencoder}

We are now able to introduce a particular architecture for the model. Let us consider a fixed (i.e., parameter-free) prior for the latent variable given by an isotropic multivariate Gaussian 
\begin{equation}
    p_\theta(z) = \cN(z;0,I).
\end{equation} 

Furthermore, we will model the decoder $p_\theta(x \mid z)$ as a multivariate Gaussian (in the case of continuous data) or a Bernoulli (in the case of binary data), both with parameters $\theta$ given by a neural network that takes $z$ as input. Though the posterior $p_\theta(z \mid x)$ is intractable, given that the prior on the latent variable is Gaussian, we will choose a Gaussian variational approximation for the posterior, i.e., the encoder, given by
\begin{equation}
    q_\phi(z \mid x) = \cN(z \mid \mu, \sigma^2 I),
    \label{eq:VAE_encoder}
\end{equation}
were the variational parameters  $\mu, \sigma^2$ are also the output of a neural network taking $x$ as input. 

Note that in this case implementing the reparametrisation trick is straightforward due to the Gaussian assumption for the encoder: sampling $z\sim\cN(0,I)$, and assigning $z = g_\phi(x, \epsilon) = \mu + \sigma\epsilon$ we obtain samples from eq.~\eqref{eq:VAE_encoder}. Since in this case both the prior $p_\theta(z)$ and the enconder $q_\phi(z \mid x)$ are Gaussian, the KL term in the ELBO can be computed explicitely, meaning that only the \emph{expected reconstruction error} needs to be approximated via Monte Carlo. 

The resulting objective for a single datapoint $x_n$ is
\begin{equation}
\elbo(\theta,\phi,x_n)
\simeq 
\frac{1}{L}\sum_{l=1}^{L}
\log p_\theta\!\left(
x_n \mid 
z_n
\right)
-
\frac{1}{2}\sum_{j=1}^{d}
\left(
\mu_j(x_n)^2 + \sigma_j(x_n)^2
- \log \sigma_j(x_n)^2 - 1
\right),
\end{equation}
where $z_n^{(l)} =\mu_\phi(x_n) + \sigma_\phi(x_n)\epsilon^{(l)}$ and $\epsilon^{(l)} \sim \mathcal{N}(0,I)$.


\section{Generative Adversarial Networks}

\section{Transformers}

\section*{Exercises}